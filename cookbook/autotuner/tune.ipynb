{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 大模型推理配置自动推荐\n",
    "\n",
    "千帆平台上的模型提供了大量参数可供用户调整，而这些参数设置会直接影响到模型表现，并且在不同场景下，最优的配置也不尽相同。\n",
    "\n",
    "针对这个问题，SDK 提供了推理配置自动推荐的功能，只需要提供目标场景的数据集及评估方式，设定搜索空间，SDK 就可以根据以上信息推荐出参数的配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import qianfan\n",
    "\n",
    "os.environ[\"QIANFAN_ACCESS_KEY\"] = \"your access key\"\n",
    "os.environ[\"QIANFAN_SECRET_KEY\"] = \"your secret key\"\n",
    "# 由于后续在调优配置的过程中会并发请求模型，建议限制 QPS 和重试次数，避免调用失败\n",
    "os.environ[\"QIANFAN_QPS_LIMIT\"] = \"3\"\n",
    "os.environ[\"QIANFAN_LLM_API_RETRY_COUNT\"] = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "\n",
    "具体而言，在获取推荐配置前，需要先准备：\n",
    "\n",
    "- 数据集：根据目标场景准备一定量的数据\n",
    "- 评估方式：根据目标场景，选择待优化的指标，并提供评估函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集使用的是千帆 SDK 中提供的 `Dataset` 模块，可以直接加载本地的数据集文件，也可以使用平台上预置的或者自行上传的数据集，具体加载方式参考 [文档](https://github.com/baidubce/bce-qianfan-sdk/blob/main/docs/dataset.md)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 19:51:40] dataset.py:389 [t:139919663059200]: no data source was provided, construct\n",
      "[INFO] [03-28 19:51:40] dataset.py:257 [t:139919663059200]: construct a file data source from path: ./example.jsonl, with args: {'input_columns': ['prompt'], 'reference_column': 'response'}\n",
      "[INFO] [03-28 19:51:40] file.py:276 [t:139919663059200]: use format type FormatType.Jsonl\n",
      "[INFO] [03-28 19:51:40] utils.py:258 [t:139919663059200]: start to get memory_map from /root/.qianfan_cache/dataset/root/work/bce-qianfan-sdk/cookbook/autotuner/example.arrow\n",
      "[INFO] [03-28 19:51:40] utils.py:233 [t:139919663059200]: has got a memory-mapped table\n"
     ]
    }
   ],
   "source": [
    "from qianfan.dataset import Dataset\n",
    "\n",
    "dataset = Dataset.load(\n",
    "    data_file=\"./example.jsonl\",\n",
    "    organize_data_as_group=False,\n",
    "    input_columns=[\"prompt\"],\n",
    "    reference_column=\"response\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "评估采用的 SDK 提供的 Evaluator 模块，基于 Evaluator 实现 evaluate 方法即可。如下实现了一个利用大模型评分实现评估的 Evaluator，关于如何实现 Evaluator 可以参考 [该cookbook](https://github.com/baidubce/bce-qianfan-sdk/blob/main/cookbook/evaluation/local_eval_with_qianfan.ipynb)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qianfan.evaluation.evaluator import LocalEvaluator\n",
    "from qianfan import ChatCompletion\n",
    "from qianfan.common.prompt.prompt import Prompt\n",
    "from qianfan.utils.pydantic import Field\n",
    "\n",
    "from typing import Optional, Union, Any, Dict, List\n",
    "import re\n",
    "import json\n",
    "\n",
    "class LocalJudgeEvaluator(LocalEvaluator):\n",
    "    model: Optional[ChatCompletion] = Field(default=None, description=\"model object\")\n",
    "    eval_prompt: Prompt = Field(\n",
    "        default=Prompt(\n",
    "            template=\"\"\"你需要扮演一个裁判的角色，对一段角色扮演的对话内容进行打分，你需要考虑这段文本中的角色沉浸度和对话文本的通畅程度。你可以根据以下规则来进行打分，你可以阐述你对打分标准的理解后再给出分数：\n",
    "                \"4\":完全可以扮演提问中的角色进行对话，回答完全符合角色口吻和身份，文本流畅语句通顺\n",
    "                \"3\":扮演了提问中正确的角色，回答完全符合角色口吻和身份，但文本不流畅或字数不满足要求\n",
    "                \"2\":扮演了提问中正确的角色，但是部分语句不符合角色口吻和身份，文本流畅语句通顺\n",
    "                \"1\":能够以角色的口吻和身份进行一部分对话，和角色设定有一定偏差，回答内容不流畅，或不满足文本字数要求\n",
    "                \"0\":扮演了错误的角色，没有扮演正确的角色，角色设定和提问设定差异极大，完全不满意\n",
    "                你的回答需要以json代码格式输出：\n",
    "                ```json\n",
    "                {\"modelA\": {\"justification\": \"此处阐述对打分标准的理解\", \"score\": \"此处填写打分结果\"}}\n",
    "                ```\n",
    "\n",
    "                现在你可以开始回答了：\n",
    "                问题：{{input}}\n",
    "                ---\n",
    "                modelA回答：{{output}}\n",
    "                ---\"\"\",\n",
    "            identifier=\"{{}}\",\n",
    "        ),\n",
    "        description=\"evaluation prompt\",\n",
    "    )\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "    def evaluate(\n",
    "        self, input: Union[str, List[Dict[str, Any]]], reference: str, output: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        score = 0\n",
    "        try:\n",
    "            # 渲染评估用的 prompt，传入输入、模型输出和参考答案\n",
    "            p, _ = self.eval_prompt.render(\n",
    "                **{\n",
    "                    \"input\": \"\\n\".join([i[\"content\"] for i in input[1:]]),\n",
    "                    \"output\": output,\n",
    "                    \"expect\": reference,\n",
    "                }\n",
    "            )\n",
    "            # 请求模型进行评估\n",
    "            r = self.model.do(messages=[{\"role\": \"user\", \"content\": p}])\n",
    "            content = r[\"result\"]\n",
    "            # 提取出 json 格式的评估结果\n",
    "            regex = re.compile(\"\\`\\`\\`json(.*)\\`\\`\\`\", re.MULTILINE | re.DOTALL)\n",
    "\n",
    "            u = regex.findall(content)\n",
    "\n",
    "            if len(u) == 0:\n",
    "                score = 0\n",
    "            else:\n",
    "                score = float(json.loads(u[0])[\"modelA\"][\"score\"])\n",
    "        except Exception as e:\n",
    "            score = 0\n",
    "        # 返回评估结果，这里字段需与后续推荐配置时设定的评估字段一致\n",
    "        return {\"score\": score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取推荐配置\n",
    "\n",
    "为了获取推荐配置，还需要设置一个超参搜索空间，千帆 SDK 提供了如下表示搜索空间的类：\n",
    "\n",
    "- `Uniform`：表示一个均匀分布的搜索空间，包含两个参数 `low` 和 `high`，分别表示下界和上界。\n",
    "- `Categorical`：表示一个离散的搜索空间，包含一个参数 `choices`，表示一组候选值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qianfan.autotuner.space import Uniform, Categorical\n",
    "\n",
    "search_space = {\n",
    "    \"temperature\": Uniform(0.01, 0.99),  # 设定temperature的范围\n",
    "    \"model\": Categorical([\"ERNIE-Speed\", \"ERNIE-Bot-turbo\"]),  # 设定model的取值范围\n",
    "    # 更多其他参数也可以按同样方式设定\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "之后就可以执行推荐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [03-28 19:51:48] launcher.py:98 [t:139919663059200]: turn 0 started...\n",
      "[INFO] [03-28 19:51:48] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.09690927085239126, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:48] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.09690927085239126, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.006291279155347083, 'avg_tokens_per_second': 11762.313859035477, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.06762361526489258}\n",
      "[INFO] [03-28 19:51:48] launcher.py:98 [t:139919663059200]: turn 1 started...\n",
      "[INFO] [03-28 19:51:48] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.19800228185518776, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:48] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.19800228185518776, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005685189531909095, 'avg_tokens_per_second': 13016.27669309218, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.06035017967224121}\n",
      "[INFO] [03-28 19:51:48] launcher.py:98 [t:139919663059200]: turn 2 started...\n",
      "[INFO] [03-28 19:51:48] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.5254985801654533, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:48] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.5254985801654533, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005813757785492473, 'avg_tokens_per_second': 12728.42845029733, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.06338071823120117}\n",
      "[INFO] [03-28 19:51:48] launcher.py:98 [t:139919663059200]: turn 3 started...\n",
      "[INFO] [03-28 19:51:48] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.9786185332987217, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:48] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:48] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.9786185332987217, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005126856060491668, 'avg_tokens_per_second': 14433.797073074715, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.06149864196777344}\n",
      "[INFO] [03-28 19:51:49] launcher.py:98 [t:139919663059200]: turn 4 started...\n",
      "[INFO] [03-28 19:51:49] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.7054783102247112, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:49] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.7054783102247112, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005222158092591498, 'avg_tokens_per_second': 14170.386780320065, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.05957603454589844}\n",
      "[INFO] [03-28 19:51:49] launcher.py:98 [t:139919663059200]: turn 5 started...\n",
      "[INFO] [03-28 19:51:49] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.9441158353498488, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:49] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.9441158353498488, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.0054768044501543045, 'avg_tokens_per_second': 13511.528606414842, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.06003212928771973}\n",
      "[INFO] [03-28 19:51:49] launcher.py:98 [t:139919663059200]: turn 6 started...\n",
      "[INFO] [03-28 19:51:49] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.01250276815284332, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:49] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.01250276815284332, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005435951054096222, 'avg_tokens_per_second': 13613.073271555275, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.06026744842529297}\n",
      "[INFO] [03-28 19:51:49] launcher.py:98 [t:139919663059200]: turn 7 started...\n",
      "[INFO] [03-28 19:51:49] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.5042003065448804, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:49] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.5042003065448804, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005643277118603389, 'avg_tokens_per_second': 13112.948105287038, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.0601963996887207}\n",
      "[INFO] [03-28 19:51:49] launcher.py:98 [t:139919663059200]: turn 8 started...\n",
      "[INFO] [03-28 19:51:49] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.10698625241380579, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:49] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.10698625241380579, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005124866755472289, 'avg_tokens_per_second': 14439.399799220815, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.059090375900268555}\n",
      "[INFO] [03-28 19:51:49] launcher.py:98 [t:139919663059200]: turn 9 started...\n",
      "[INFO] [03-28 19:51:49] launcher.py:99 [t:139919663059200]: suggested config list: [{'temperature': 0.3676493569700079, 'model': 'ERNIE-Speed'}]\n",
      "[INFO] [03-28 19:51:49] dataset.py:883 [t:139919663059200]: list local dataset data by None\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:377 [t:139919663059200]: async requesting llm api endpoint: /chat/ernie_speed\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918580074240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918605252352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918588466944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] openapi_requestor.py:336 [t:139918596859648]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [03-28 19:51:49] launcher.py:104 [t:139919663059200]: config: {'temperature': 0.3676493569700079, 'model': 'ERNIE-Speed'}, metrics: {'score': 0.0, 'avg_prompt_tokens': 7.0, 'avg_completion_tokens': 67.0, 'avg_total_tokens': 74.0, 'avg_req_latency': 0.005148769873711798, 'avg_tokens_per_second': 14372.365014374334, 'avg_cost': 0.0005640000000000002, 'total_cost': 0.0050760000000000015, 'success_rate': 1.0, 'total_time': 0.06065654754638672}\n",
      "[INFO] [03-28 19:51:49] launcher.py:87 [t:139919663059200]: max turn reached: 10\n",
      "[INFO] [03-28 19:51:49] launcher.py:92 [t:139919663059200]: tuning finished!\n",
      "[INFO] [03-28 19:51:49] launcher.py:94 [t:139919663059200]: best config: {'temperature': 0.09690927085239126, 'model': 'ERNIE-Speed'}\n"
     ]
    }
   ],
   "source": [
    "import qianfan.autotuner\n",
    "\n",
    "context = await qianfan.autotuner.run(\n",
    "    search_space=search_space,\n",
    "    dataset=dataset,\n",
    "    evaluator=LocalJudgeEvaluator(\n",
    "        model=ChatCompletion(model=\"ERNIE-Bot-4\")\n",
    "    ),\n",
    "    # 以下均为可选参数\n",
    "    suggestor=\"random\",  # 搜索算法，目前仅支持 \"random\"，更多算法敬请期待\n",
    "    cost_budget=1,       # 设定整个流程的预算，达到预算则终止流程，单位为 “元”\n",
    "    metrics=\"score\",     # 设定评估指标字段，与 Evaluator 输出对应\n",
    "    mode=\"max\",          # 设定评估指标最大化还是最小化\n",
    "    repeat=3,            # 重复推理次数，用于减少大模型输出随机性对结果准确性的干扰\n",
    "    max_turn=10,         # 设定最大尝试次数\n",
    "    max_time=3600,       # 设定最大尝试时间，单位为秒\n",
    "    log_dir= \"./log\",    # 日志目录\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回的结果是一个 `Context` 对象，其中包含了整个搜索过程的所有上下文信息，例如可以通过如下方式获得搜索的最佳参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 0.22907952895473882, 'model': 'ERNIE-Speed'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个参数直接用于推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = qianfan.ChatCompletion().do(messages=[{\n",
    "    \"role\": \"user\",\n",
    "    \"messages\": \"请扮演一个角色，然后说一句话\"\n",
    "}], **context.best)\n",
    "\n",
    "print(chat['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context 中也包含了整个过程中尝试的记录，可以获取某一轮某一组配置的评估结果等信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'avg_prompt_tokens': 7.0,\n",
       " 'avg_completion_tokens': 67.0,\n",
       " 'avg_total_tokens': 74.0,\n",
       " 'avg_req_latency': 0.005900350709756215,\n",
       " 'avg_tokens_per_second': 12541.627377783016,\n",
       " 'avg_cost': 0.0005640000000000002,\n",
       " 'total_cost': 0.0050760000000000015,\n",
       " 'success_rate': 1.0,\n",
       " 'total_time': 0.06302499771118164}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.history[0][0].metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 如何使用千帆 Python SDK 搭配预置大模型服务进行批量推理\n",
    "\n",
    "在 0.2.8 版本中，千帆 Python SDK 增加了对批量推理的支持，使用该功能需要视情况开通千帆的模型服务，确保您的账号可以调用您想进行批量推理的服务。\n",
    "\n",
    "# 准备工作\n",
    "\n",
    "在开始之前，请确保你的千帆 Python SDK 已经升级到了 0.2.8 及以上版本。\n",
    "nest_asyncio 是一个异步库，用于支持 Python SDK 的异步推理功能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# ! pip install -U \"qianfan>=0.2.8\"\n",
    "# ! pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并且在环境变量中设置好 Access Key 与 Secret Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T04:41:29.790082Z",
     "start_time": "2024-02-23T04:41:29.549765Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "from qianfan.utils import enable_log\n",
    "\n",
    "os.environ['QIANFAN_ACCESS_KEY'] = 'your_access_key'\n",
    "os.environ['QIANFAN_SECRET_KEY'] = 'your_secret_key'\n",
    "\n",
    "os.environ[\"QIANFAN_QPS_LIMIT\"] = \"1\"\n",
    "os.environ['QIANFAN_LLM_API_RETRY_COUNT'] = \"3\"\n",
    "\n",
    "# 选择打印出来的日志等级，目前打印出 info 级别\n",
    "enable_log(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 正文\n",
    "\n",
    "为了开始批量推理，我们首先需要获取到用于做批量推理输入的数据集文件，并且指定用做推理输入的列名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T04:41:39.847158Z",
     "start_time": "2024-02-23T04:41:39.321783Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [02-23 12:41:39] dataset.py:489 [t:8599036544]: no data source was provided, construct\n",
      "[INFO] [02-23 12:41:39] dataset.py:358 [t:8599036544]: construct a file data source from path: data_file/qa_pair.csv, with args: {'input_columns': ['prompt'], 'reference_column': 'response'}\n",
      "[INFO] [02-23 12:41:39] file.py:165 [t:8599036544]: use format type FormatType.Csv\n",
      "[INFO] [02-23 12:41:39] dataset.py:934 [t:8599036544]: list local dataset data by 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '地球的自转周期是多久？', 'response': '大约24小时'}\n"
     ]
    }
   ],
   "source": [
    "from qianfan.dataset import Dataset\n",
    "\n",
    "dataset_file_path = \"data_file/qa_pair.csv\"\n",
    "dataset_input_column_list = [\"prompt\"]\n",
    "\n",
    "# 预期输出列列名，当数据集为对话类数据集时必填，为非对话数据集时选填。\n",
    "# 对应列的数据会在推理结果中出现\n",
    "reference_column = \"response\"\n",
    "\n",
    "ds = Dataset.load(data_file=dataset_file_path, input_columns=dataset_input_column_list, reference_column=reference_column)\n",
    "\n",
    "# 预览数据格式\n",
    "print(ds.list(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在导入之后，用户可以根据自己的需求，传入不同的参数来使用不同的方式进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T01:59:04.858288Z",
     "start_time": "2024-02-23T01:55:41.148258Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13195636736]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13229215744]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13363531776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13279584256]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13246005248]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13329952768]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13346742272]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13296373760]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13262794752]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13212426240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13397110784]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13430689792]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13413900288]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13313163264]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13481058304]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13497847808]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13380321280]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13548216320]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13447479296]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13514637312]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13581795328]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13464268800]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13565005824]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13531426816]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13615374336]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13598584832]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13665742848]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13632163840]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13682532352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13749690368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13699321856]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13648953344]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13716111360]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13732900864]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13884006400]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13766479872]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13800058880]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13833637888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13917585408]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13951164416]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13900795904]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13850427392]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13816848384]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13867216896]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13783269376]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14001532928]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14018322432]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13967953920]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13934374912]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:13984743424]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14120165376]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14035111936]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14051901440]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14085480448]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14153744384]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14103375872]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14068690944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14170533888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14187323392]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14136954880]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14204112896]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14220902400]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14237691904]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14288060416]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14271270912]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14254481408]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14455955456]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14321639424]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14355218432]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14422376448]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14338428928]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14372007936]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14304849920]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14388797440]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14472744960]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14439165952]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14405586944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14489534464]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14523113472]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14607060992]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14506323968]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14539902976]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14623850496]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14573481984]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14556692480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:55:41] openapi_requestor.py:244 [t:14590271488]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] base.py:89 [t:14556692480]: All tasks finished, exeutor will be shutdown\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14657429504]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14640640000]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13229215744]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13246005248]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14674219008]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13262794752]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13195636736]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13296373760]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13363531776]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13212426240]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13313163264]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13329952768]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13346742272]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13279584256]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13430689792]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13397110784]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13413900288]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13497847808]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13380321280]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13531426816]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13514637312]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13581795328]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13447479296]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13598584832]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13464268800]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13548216320]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13615374336]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13632163840]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13481058304]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13565005824]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13699321856]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13648953344]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13665742848]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13766479872]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13682532352]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13783269376]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13716111360]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13749690368]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13732900864]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13816848384]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13800058880]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13833637888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13934374912]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13917585408]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13884006400]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13850427392]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13900795904]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13867216896]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14035111936]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13951164416]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13967953920]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:13984743424]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14018322432]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14001532928]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14051901440]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14068690944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14085480448]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14153744384]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14120165376]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14187323392]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14103375872]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14237691904]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14170533888]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14204112896]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14220902400]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14136954880]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14254481408]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14271270912]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14288060416]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14304849920]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14338428928]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14321639424]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14405586944]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14372007936]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14489534464]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14439165952]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14422376448]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14388797440]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14506323968]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14355218432]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14472744960]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14455955456]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14523113472]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14556692480]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14539902976]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:57:24] openapi_requestor.py:244 [t:14573481984]: requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 09:59:04] base.py:89 [t:14539902976]: All tasks finished, exeutor will be shutdown\n"
     ]
    }
   ],
   "source": [
    "#-# cell_skip\n",
    "# 用户可以设置 service_model 为自己想要的模型名，来直接对数据进行批量推理，以 EB 4 为例\n",
    "result = ds.test_using_llm(service_model=\"ERNIE-Bot-4\")\n",
    "\n",
    "# 用户还可以设置 service_endpoint 来使用预置或自己的服务。\n",
    "result = ds.test_using_llm(service_endpoint=\"completions_pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果用户有异步请求的需求，还可以使用 `atest_using_llm` 来进行异步批量推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T04:42:55.253181Z",
     "start_time": "2024-02-23T04:41:42.768227Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] oauth.py:229 [t:8599036544]: trying to refresh access_token for ak `rRlk1M***`\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:42] openapi_requestor.py:275 [t:8599036544]: async requesting llm api endpoint: /chat/completions_pro\n",
      "[INFO] [02-23 12:41:43] oauth.py:243 [t:8599036544]: sucessfully refresh access_token for ak `rRlk1M***`\n",
      "[ERROR] [02-23 12:41:54] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-x7t1cyt3dv failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:41:54] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-zzu1pxse5e failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:03] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-r85iihvy8r failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:15] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-wb3gt88fjd failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:19] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-dpkyzvevy7 failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:20] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-dy2v4fw3yb failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:28] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-8f3qa8ii3s failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:31] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-8aks25c4qg failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:31] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-z58uqtsg22 failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:38] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-qtptzsws15 failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:41] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-86x9w1pjc2 failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:42] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-0rtcj0a2tq failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:42] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-i8x91us4bb failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:50] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-zcb21cicw3 failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:51] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-t85uee7d46 failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:51] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-xmejbx7t1h failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n",
      "[ERROR] [02-23 12:42:52] openapi_requestor.py:219 [t:8599036544]: api request req_id: as-gndph74w1b failed with error code: 336501, err msg: Rate limit reached for RPM, please check https://cloud.baidu.com/doc/WENXINWORKSHOP/s/tlmyncueh\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/8c/vcrpfgjx0bq3cdry4vw4tvqm0000gn/T/ipykernel_8717/3266099216.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mnest_asyncio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0masyncio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0matest_using_llm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mservice_endpoint\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"completions_pro\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(main, debug)\u001B[0m\n\u001B[1;32m     33\u001B[0m         \u001B[0mtask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0masyncio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_future\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmain\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     34\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 35\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mloop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_until_complete\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     36\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     37\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mtask\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py\u001B[0m in \u001B[0;36mrun_until_complete\u001B[0;34m(self, future)\u001B[0m\n\u001B[1;32m     81\u001B[0m                 \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_log_destroy_pending\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     82\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdone\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 83\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_run_once\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     84\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stopping\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     85\u001B[0m                     \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nest_asyncio.py\u001B[0m in \u001B[0;36m_run_once\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    104\u001B[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001B[1;32m    105\u001B[0m             else None)\n\u001B[0;32m--> 106\u001B[0;31m         \u001B[0mevent_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_selector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mselect\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    107\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_process_events\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mevent_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/selectors.py\u001B[0m in \u001B[0;36mselect\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    560\u001B[0m             \u001B[0mready\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    561\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 562\u001B[0;31m                 \u001B[0mkev_list\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_selector\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcontrol\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmax_ev\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    563\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0mInterruptedError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    564\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mready\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "result = asyncio.run(ds.atest_using_llm(service_endpoint=\"completions_pro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拿到的 `result` 对象也是一个 `Dataset` 对象，可以继续使用千帆 Python SDK 进行后续处理，或者直接保存到本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [12-28 13:05:47] dataset.py:981 [t:8120441664]: list local dataset data by 0\n",
      "[INFO] [12-28 13:05:47] dataset.py:525 [t:8120441664]: no destination data source was provided, construct\n",
      "[INFO] [12-28 13:05:47] dataset.py:337 [t:8120441664]: construct a file data source from path: output_file.csv, with args: {'is_download_to_local': False}\n",
      "[INFO] [12-28 13:05:47] data_source.py:280 [t:8120441664]: use format type FormatType.Csv\n",
      "[INFO] [12-28 13:05:47] dataset.py:233 [t:8120441664]: export as format: FormatType.Csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '地球的自转周期是多久？', 'input_prompt': '地球的自转周期是多久？', 'llm_output': '地球的自转周期是**24小时**，即一天一夜。这是地球绕自身轴线旋转一周所需的时间。', 'expected_output': '大约24小时'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(result.list(0))\n",
    "      \n",
    "dataset_save_file_path = \"output_file.csv\"\n",
    "\n",
    "result.save(data_file=dataset_save_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对模型进行批量推理\n",
    "\n",
    "除了对 `Service` 进行批量推理，我们也可以对 `Model` 进行批量推理\n",
    "\n",
    "在对 `Model` 进行批量推理时，请先确认用到的数据集已经在千帆平台上发布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [01-05 16:31:19] dataset.py:466 [t:8120441664]: no data source was provided, construct\n",
      "[INFO] [01-05 16:31:19] dataset.py:350 [t:8120441664]: construct a qianfan data source from existed id: 44478, with args: {'is_download_to_local': False}\n",
      "[INFO] [01-05 16:31:20] dataset.py:150 [t:8120441664]: a cloud dataset has been created\n",
      "[INFO] [01-05 16:31:20] dataset_utils.py:353 [t:8120441664]: start to create evaluation task in model\n",
      "[INFO] [01-05 16:31:24] dataset_utils.py:315 [t:8120441664]: start to polling status of evaluation task 2817\n",
      "[INFO] [01-05 16:31:24] dataset_utils.py:322 [t:8120441664]: current eval_state: Pending\n",
      "[INFO] [01-05 16:31:54] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:32:25] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:32:55] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:33:25] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:33:56] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:34:26] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:34:57] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:35:28] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:35:58] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:36:28] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:36:59] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:37:29] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:37:59] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:38:30] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:39:00] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:39:30] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:40:01] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:40:31] dataset_utils.py:322 [t:8120441664]: current eval_state: Doing\n",
      "[INFO] [01-05 16:41:01] dataset_utils.py:322 [t:8120441664]: current eval_state: DoingWithManualBegin\n",
      "[INFO] [01-05 16:41:01] dataset_utils.py:340 [t:8120441664]: get result dataset id 44629\n",
      "[INFO] [01-05 16:41:01] dataset.py:466 [t:8120441664]: no data source was provided, construct\n",
      "[INFO] [01-05 16:41:01] dataset.py:350 [t:8120441664]: construct a qianfan data source from existed id: 44629, with args: {}\n",
      "[INFO] [01-05 16:41:02] data_source.py:1096 [t:8120441664]: start to fetch dataset cache because is_download_to_local is set\n",
      "[INFO] [01-05 16:41:02] data_source.py:697 [t:8120441664]: no cache was found, download cache\n",
      "[INFO] [01-05 16:41:03] data_source.py:589 [t:8120441664]: get dataset info succeeded for dataset id 44629\n",
      "[INFO] [01-05 16:41:03] data_source.py:596 [t:8120441664]: start to export dataset\n",
      "[INFO] [01-05 16:41:03] data_source.py:600 [t:8120441664]: create dataset export task successfully\n",
      "[INFO] [01-05 16:41:05] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:06] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:08] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:08] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:10] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:11] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:13] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:13] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:15] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:15] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:17] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:18] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:20] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:20] data_source.py:610 [t:8120441664]: export succeed\n",
      "[INFO] [01-05 16:41:20] data_source.py:565 [t:8120441664]: get export records succeeded for dataset id 44629\n",
      "[INFO] [01-05 16:41:20] data_source.py:581 [t:8120441664]: latest dataset with time2024-01-05 16:41:19 for dataset 44629\n",
      "[INFO] [01-05 16:41:20] data_source.py:626 [t:8120441664]: start to download dataset zip from url https://bj.bcebos.com/easydata-upload/export_local/%E8%AF%84%E4%BC%B0%E4%BB%BB%E5%8A%A1_model_run_LzzKiuZ4Q0_%E7%BB%93%E6%9E%9C%E9%9B%86_f23d10V1_20240105_164103.zip?authorization=bce-auth-v1%2F50c8bb753dcb4e1d8646bb1ffefd3503%2F2024-01-05T08%3A41%3A20Z%2F3600%2Fhost%2Fcb1ed680558c418f42272cbe1269b3a980db021e9140c4225eeb0d5c58eb34aa\n",
      "[INFO] [01-05 16:41:21] data_source.py:645 [t:8120441664]: download dataset zip to .qianfan_dataset_cache/36737/44629/1/bin.zip succeeded\n",
      "[INFO] [01-05 16:41:21] data_source.py:672 [t:8120441664]: unzip dataset to path .qianfan_dataset_cache/36737/44629/1/content successfully\n",
      "[INFO] [01-05 16:41:21] data_source.py:676 [t:8120441664]: write dataset info to path .qianfan_dataset_cache/36737/44629/1/info.json successfully\n",
      "[INFO] [01-05 16:41:21] data_source.py:725 [t:8120441664]: dataset cache is outdated, update cache\n",
      "[INFO] [01-05 16:41:22] data_source.py:589 [t:8120441664]: get dataset info succeeded for dataset id 44629\n",
      "[INFO] [01-05 16:41:22] data_source.py:565 [t:8120441664]: get export records succeeded for dataset id 44629\n",
      "[INFO] [01-05 16:41:22] data_source.py:581 [t:8120441664]: latest dataset with time2024-01-05 16:41:19 for dataset 44629\n",
      "[INFO] [01-05 16:41:22] data_source.py:596 [t:8120441664]: start to export dataset\n",
      "[INFO] [01-05 16:41:23] data_source.py:600 [t:8120441664]: create dataset export task successfully\n",
      "[INFO] [01-05 16:41:25] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:25] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:27] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:27] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:29] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:30] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:32] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:32] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:34] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:35] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:37] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:37] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:39] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:40] data_source.py:613 [t:8120441664]: export status: 1, keep polling\n",
      "[INFO] [01-05 16:41:42] data_source.py:605 [t:8120441664]: polling export task status\n",
      "[INFO] [01-05 16:41:42] data_source.py:610 [t:8120441664]: export succeed\n",
      "[INFO] [01-05 16:41:42] data_source.py:565 [t:8120441664]: get export records succeeded for dataset id 44629\n",
      "[INFO] [01-05 16:41:42] data_source.py:581 [t:8120441664]: latest dataset with time2024-01-05 16:41:40 for dataset 44629\n",
      "[INFO] [01-05 16:41:42] data_source.py:626 [t:8120441664]: start to download dataset zip from url https://bj.bcebos.com/easydata-upload/export_local/%E8%AF%84%E4%BC%B0%E4%BB%BB%E5%8A%A1_model_run_LzzKiuZ4Q0_%E7%BB%93%E6%9E%9C%E9%9B%86_f23d10V1_20240105_164122.zip?authorization=bce-auth-v1%2F50c8bb753dcb4e1d8646bb1ffefd3503%2F2024-01-05T08%3A41%3A42Z%2F3600%2Fhost%2Fe94091781afdfbb5dd8acbbd52697410f579db8959f78d5711eb0571823e9487\n",
      "[INFO] [01-05 16:41:43] data_source.py:645 [t:8120441664]: download dataset zip to .qianfan_dataset_cache/36737/44629/1/bin.zip succeeded\n",
      "[INFO] [01-05 16:41:43] data_source.py:672 [t:8120441664]: unzip dataset to path .qianfan_dataset_cache/36737/44629/1/content successfully\n",
      "[INFO] [01-05 16:41:43] data_source.py:676 [t:8120441664]: write dataset info to path .qianfan_dataset_cache/36737/44629/1/info.json successfully\n",
      "[INFO] [01-05 16:41:43] dataset.py:905 [t:8120441664]: list local dataset data by 0\n",
      "[INFO] [01-05 16:41:43] dataset.py:905 [t:8120441664]: list local dataset data by None\n",
      "[INFO] [01-05 16:41:43] dataset.py:905 [t:8120441664]: list local dataset data by 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '人类的基本单位是什么？', 'input_prompt': '人类的基本单位是什么？', 'llm_output': '人类社会学界、群落、组织、团队、组织、机构、公司、政府、公司、家庭、家庭、军队、学校和家庭、村落、门派、门派、门派、门派、门派是人类的集合。\\n\\n\\n\\n\\n人类的基本结构层次单位是集合名词，人类的基本单位是集合名词，人类的基本单位是集合名词，人类的基本单位是集合名词，人类的基本单位是个人。\\n\\n\\n人类的基本单位是基本单位是个人。', 'expected_output': '人类'}\n"
     ]
    }
   ],
   "source": [
    "cloud_dataset_id = \"dataset_id\"\n",
    "\n",
    "qianfan_ds = Dataset.load(qianfan_dataset_id=cloud_dataset_id, is_download_to_local=False)\n",
    "\n",
    "result = qianfan_ds.test_using_llm(model_version_id=\"amv-qb8ijukaish3\")\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进阶能力\n",
    "\n",
    "在调用 `test_using_llm` 时，用户还可以传入一些额外参数来支持额外的功能，比如设置 Prompt 模板，设置人设字段，或者传入大模型调用时的超参数\n",
    "\n",
    "当对服务进行非对话类推理时，用户可以传入 `prompt_template` 参数来传递一个 Prompt 模板。`prompt_template` 是一个千帆 Python SDK 的 `Prompt` 对象，用户可以通过设置 `Prompt` 对象的 `template` 成员来自定义被用于推理的模板，模板渲染出来的内容将会被作为最终输入提交给大模型。以示例数据集为例，我们可以这么指定一个模板："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qianfan.common import Prompt\n",
    "\n",
    "prompt = Prompt(template=\"请你就以下问题进行回答: {prompt}\")\n",
    "\n",
    "# 传递给函数\n",
    "result = ds.test_using_llm(service_model=\"ERNIE-Bot-4\", prompt_template=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除此之外，用户还可以传入 `system_prompt` 参数来指定对话中大模型需要遵守的人设"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ds.test_using_llm(service_model=\"ERNIE-Bot-4\", system_prompt=\"人设 prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用户在进行批量推理时，还可以直接在 test_using_llm 中传入模型支持的超参数，例如我们可以这么设置模型的 `temperature` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ds.test_using_llm(service_model=\"ERNIE-Bot-4\", system_prompt=\"人设 prompt\", temperature=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
